{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during transcription: Error code: 401 - {'error': {'message': 'Incorrect API key provided: api_key. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Define mp4 and mp3 file paths\n",
    "mp4_file = '../video/Scoring -20250219_153216-Meeting Recording.mp4'\n",
    "mp3_file = mp4_file.replace('.mp4', '.mp3')  # Define mp3_file variable\n",
    "\n",
    "# # Convert mp4 to mp3 using ffmpeg directly\n",
    "# import subprocess\n",
    "\n",
    "# try:\n",
    "#     # Use ffmpeg to extract audio from video\n",
    "#     subprocess.call(['ffmpeg', '-i', mp4_file, '-q:a', '0', '-map', 'a', mp3_file, '-y'])\n",
    "#     print(f\"Successfully converted {mp4_file} to {mp3_file}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error converting file: {e}\")\n",
    "\n",
    "\n",
    "# Now use OpenAI's Whisper for transcription\n",
    "from openai import OpenAI\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key='api_key')\n",
    "\n",
    "# Use the mp3_file we created earlier\n",
    "try:\n",
    "    audio_file = open(mp3_file, \"rb\")\n",
    "    transcription = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\", \n",
    "        file=audio_file\n",
    "    )\n",
    "    print(transcription.text)\n",
    "    # Don't forget to close the file\n",
    "    audio_file.close()\n",
    "except Exception as e:\n",
    "    print(f\"Error during transcription: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result from OpenAI whisper1:\n",
    "`Ня спуснах сега, вие трябва да добрите ме. М? Да. Не се мютнахте нещо. То самата мютва. А, да. То те мютва, да, да. Кирил, Тони, мютнави. Мютнави. Не ме виждаме. Не се управи при мен. На микрофонците си цъкнах и те ме попита тогава. Защото не ми излигаше по-папа. Са гласа ми си. Ето и турията е готова. Ами не, на мен ми се прави в някаква страница, която постоя дада ерор. Да, и на мен първо, ама я затворих веднага и после на микрофонците си цъкнах. Ами ти, Митко, али беш записвал? Ами правихме такива срещи там по... В началото казах. Но не помня дали се пробва да ги изтеглям. Да ги даунлодвам. А тогава ще трябва да го даунлоднем и за да го процеснем. А сега кой пусна записата? Ти ли го пусна? Аз, да. Сега го спирам и ще видя дали мога да го изтегля.`\n",
    "\n",
    "Result from Azure: \n",
    "`Ня спуснах сега, вие трябва да добрите ме. Да. На сега мютнахте нещо. То самата мютва. А, да, то те мютва, да, да. Кирил, Тони, мютнави. Мютнави. Май се оправи при мене. Да. На микрофончето си цъкнах и ме попита тогава, защото не ми излизаше pop-up-а с гласа ми си. Ето и ти работи готово. Ами не, на мен ми спрати в някаква страница, в която после ми даде ерор. Да, и на мен първо, ама ми я затворих веднага и после на микрофончето си цъкнах. Матемей, това ли беше записвало? Ни правихме такива срещи там по началото, когато говорихме там. Да, но не помня дали се пробва да ги изтеглям, да ги даунлодвам. А това е, че трябва да го даунлоднем и за да го процеснем. А сега кой пусна записа? Те ли го пусна? Аз, да. Сега го спирам и ще видя дали мога да го изтегля. Аз.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying open source 'DrishtiSharma/wav2vec2-large-xls-r-300m-bg-d2' finetuneed wav2vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bulgarian Transcription:\n",
      "нязпуснах се гавие тралодобритемдана съмютнахте нищотос мотомютоа да оте мютва дадан кирилотони мютнавимют навии аз сега майсоправи примеенда даммно икрфончето си цъкнах ина попито тога мам чотне ми издияше поапасъгаиде ето ивирида отоламиледа ме  изпрати във леква страицааквкоат посвидада е роа ина мебърволама бия затворих веднага и посана ми крфончетоат цекнак  на ти митко ли беж за писва ни праихматкия сраещи там поона чалт га борихма там неноне промнядоли спрова да ги изеглям да ги дално одвамато сече тяно годално онем да го о издаголопрцеснем а сега койпоса запсетели го поста аз да ся го сдорирам иживра дри мого изтелил   аз\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoProcessor, AutoModelForCTC\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"DrishtiSharma/wav2vec2-large-xls-r-300m-bg-d2\")\n",
    "model = AutoModelForCTC.from_pretrained(\"DrishtiSharma/wav2vec2-large-xls-r-300m-bg-d2\")\n",
    "\n",
    "# Now use the Hugging Face model for transcription\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    # Load audio file and resample to 16kHz (required by the model)\n",
    "    mp3_file = '../video/Scoring -20250219_153216-Meeting Recording.mp3'\n",
    "    with open(mp3_file, 'rb') as audio:\n",
    "        audio_input, sample_rate = librosa.load(mp3_file, sr=16000)\n",
    "        \n",
    "        # Process the audio with the model\n",
    "        inputs = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs.input_values).logits\n",
    "        \n",
    "        # Get the predicted token ids\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        # Decode the token ids to text\n",
    "        transcription = processor.batch_decode(predicted_ids)[0]\n",
    "        \n",
    "        print(\"Bulgarian Transcription:\")\n",
    "        print(transcription)\n",
    "        \n",
    "except Exception as e:\n",
    "        print(f\"Error during transcription with Hugging Face model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diarization (speaker Multi Speaker Transcription)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (4064) too large for available bit count (3928)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2432) too large for available bit count (2384)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3904) too large for available bit count (3352)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3392) too large for available bit count (3352)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1632) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3776) too large for available bit count (3352)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1696) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1912)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1632) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2560) too large for available bit count (2200)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1920) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1952) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1632) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3968) too large for available bit count (3928)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1920) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3136) too large for available bit count (2776)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2368) too large for available bit count (2200)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2080) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1952) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (4000) too large for available bit count (3904)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3872) too large for available bit count (3352)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1624)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1888) too large for available bit count (1624)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Space-separated RTTM file format does not allow file URIs containing spaces (got: \"Scoring -20250219_153216-Meeting Recording\").",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# dump the diarization output to disk using RTTM format\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio.rttm\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m rttm:\n\u001b[0;32m---> 14\u001b[0m     diarization\u001b[38;5;241m.\u001b[39mwrite_rttm(rttm)\n",
      "File \u001b[0;32m~/envs/jupyter1/lib/python3.12/site-packages/pyannote/core/annotation.py:413\u001b[0m, in \u001b[0;36mAnnotation.write_rttm\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite_rttm\u001b[39m(\u001b[38;5;28mself\u001b[39m, file: TextIO):\n\u001b[1;32m    402\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Dump annotation to file using RTTM format\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m    ...     annotation.write_rttm(file)\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_rttm():\n\u001b[1;32m    414\u001b[0m         file\u001b[38;5;241m.\u001b[39mwrite(line)\n",
      "File \u001b[0;32m~/envs/jupyter1/lib/python3.12/site-packages/pyannote/core/annotation.py:378\u001b[0m, in \u001b[0;36mAnnotation._iter_rttm\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(uri, Text) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m uri:\n\u001b[1;32m    374\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpace-separated RTTM file format does not allow file URIs \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontaining spaces (got: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    377\u001b[0m     )\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m segment, _, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitertracks(yield_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(label, Text) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m label:\n",
      "\u001b[0;31mValueError\u001b[0m: Space-separated RTTM file format does not allow file URIs containing spaces (got: \"Scoring -20250219_153216-Meeting Recording\")."
     ]
    }
   ],
   "source": [
    "# instantiate the pipeline\n",
    "from pyannote.audio import Pipeline\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "  \"pyannote/speaker-diarization-3.1\",\n",
    "  use_auth_token=HF_TOKEN)\n",
    "\n",
    "# run the pipeline on an audio file\n",
    "mp3_file = '../video/Scoring -20250219_153216-Meeting Recording.mp3'\n",
    "diarization = pipeline(mp3_file)\n",
    "\n",
    "# dump the diarization output to disk using RTTM format\n",
    "with open(\"audio.rttm\", \"w\") as rttm:\n",
    "    diarization.write_rttm(rttm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<pyannote.core.annotation.Annotation at 0x7f53b9383920>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diarization with Whisper\n",
    "- careful with permissions when accessing separate files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Attempting speaker diarization...\n",
      "Initializing pipeline with token: hf_sI...QaZUB\n",
      "Successfully loaded pyannote/speaker-diarization-3.1\n",
      "Using device: cuda\n",
      "Running diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (4064) too large for available bit count (3928)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2432) too large for available bit count (2384)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3904) too large for available bit count (3352)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3392) too large for available bit count (3352)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1632) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3776) too large for available bit count (3352)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1696) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1912)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1632) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2560) too large for available bit count (2200)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1920) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1952) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1632) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3968) too large for available bit count (3928)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1920) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3136) too large for available bit count (2776)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43 speech segments with 3 speakers\n",
      "Error in diarization or transcription process: [Errno 13] Permission denied: 'ffprobe'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2368) too large for available bit count (2200)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2080) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1952) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (4000) too large for available bit count (3904)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3872) too large for available bit count (3352)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1624)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1888) too large for available bit count (1624)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/pydub/utils.py:198: RuntimeWarning: Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\n",
      "  warn(\"Couldn't find ffprobe or avprobe - defaulting to ffprobe, but may not work\", RuntimeWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_349040/2498167422.py\", line 103, in <module>\n",
      "    audio = AudioSegment.from_file(mp3_file)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/pydub/audio_segment.py\", line 728, in from_file\n",
      "    info = mediainfo_json(orig_file, read_ahead_limit=read_ahead_limit)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/pydub/utils.py\", line 274, in mediainfo_json\n",
      "    res = Popen(command, stdin=stdin_parameter, stdout=PIPE, stderr=PIPE)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/dpeykov/envs/jupyter1/lib/python3.12/subprocess.py\", line 1028, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"/home/dpeykov/envs/jupyter1/lib/python3.12/subprocess.py\", line 1963, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "PermissionError: [Errno 13] Permission denied: 'ffprobe'\n"
     ]
    }
   ],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from pyannote.audio import Pipeline\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Define mp4 and mp3 file paths\n",
    "mp4_file = '../video/Scoring -20250219_153216-Meeting Recording.mp4'\n",
    "mp3_file = mp4_file.replace('.mp4', '.mp3')\n",
    "\n",
    "# # Convert mp4 to mp3 using ffmpeg directly\n",
    "# import subprocess\n",
    "\n",
    "# try:\n",
    "#     # Use ffmpeg to extract audio from video\n",
    "#     subprocess.call(['ffmpeg', '-i', mp4_file, '-q:a', '0', '-map', 'a', mp3_file, '-y'])\n",
    "#     print(f\"Successfully converted {mp4_file} to {mp3_file}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error converting file: {e}\")\n",
    "\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# print(\"Transcribing the full audio file...\")\n",
    "# try:\n",
    "#     with open(mp3_file, \"rb\") as audio_file:\n",
    "#         transcription = client.audio.transcriptions.create(\n",
    "#             model=\"whisper-1\", \n",
    "#             file=audio_file\n",
    "#         )\n",
    "#     print(\"Full transcription:\")\n",
    "#     print(transcription.text)\n",
    "    \n",
    "#     # Save the full transcription\n",
    "#     with open(\"full_transcription.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(transcription.text)\n",
    "    \n",
    "#     print(\"\\nFull transcription saved to full_transcription.txt\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during transcription: {e}\")\n",
    "\n",
    "# Now try diarization with explicit token handling\n",
    "print(\"\\nAttempting speaker diarization...\")\n",
    "\n",
    "# Explicitly ask for the token if not set\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    print(\"HF_TOKEN environment variable not found.\")\n",
    "    hf_token = input(\"Please enter your Hugging Face token: \")\n",
    "\n",
    "try:\n",
    "    # Initialize the pipeline with explicit authentication\n",
    "    print(f\"Initializing pipeline with token: {hf_token[:5]}...{hf_token[-5:] if len(hf_token) > 10 else ''}\")\n",
    "    \n",
    "    # Try with the newer model first\n",
    "    try:\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\",\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        print(\"Successfully loaded pyannote/speaker-diarization-3.1\")\n",
    "    except Exception as e1:\n",
    "        print(f\"Error loading pyannote/speaker-diarization-3.1: {e1}\")\n",
    "        print(\"Trying with the older model...\")\n",
    "        \n",
    "        # Fall back to the older model\n",
    "        pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization\",\n",
    "            use_auth_token=hf_token\n",
    "        )\n",
    "        print(\"Successfully loaded pyannote/speaker-diarization\")\n",
    "    \n",
    "    # Verify pipeline is not None\n",
    "    if pipeline is None:\n",
    "        raise ValueError(\"Pipeline is None after initialization\")\n",
    "    \n",
    "    # Send pipeline to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    pipeline = pipeline.to(device)\n",
    "    \n",
    "    # Apply the pipeline to the audio file\n",
    "    print(\"Running diarization...\")\n",
    "    diarization = pipeline(mp3_file)\n",
    "    \n",
    "    # Extract segments with speaker information\n",
    "    segments = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        segments.append({\n",
    "            \"speaker\": speaker,\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end\n",
    "        })\n",
    "    \n",
    "    print(f\"Found {len(segments)} speech segments with {len(set(s['speaker'] for s in segments))} speakers\")\n",
    "    \n",
    "    # Transcribe audio segments with speaker labels\n",
    "    audio = AudioSegment.from_file(mp3_file)\n",
    "    \n",
    "    # Create a directory for temporary files if it doesn't exist\n",
    "    os.makedirs(\"temp_segments\", exist_ok=True)\n",
    "    \n",
    "    transcriptions = []\n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        # Extract segment audio\n",
    "        start_ms = int(segment[\"start\"] * 1000)\n",
    "        end_ms = int(segment[\"end\"] * 1000)\n",
    "        segment_audio = audio[start_ms:end_ms]\n",
    "        \n",
    "        # Save segment to temporary file\n",
    "        temp_file = f\"temp_segments/segment_{i}.mp3\"\n",
    "        segment_audio.export(temp_file, format=\"mp3\")\n",
    "        \n",
    "        # Transcribe segment\n",
    "        with open(temp_file, \"rb\") as audio_file:\n",
    "            try:\n",
    "                result = client.audio.transcriptions.create(\n",
    "                    model=\"whisper-1\",\n",
    "                    file=audio_file\n",
    "                )\n",
    "                \n",
    "                transcriptions.append({\n",
    "                    \"speaker\": segment[\"speaker\"],\n",
    "                    \"start\": segment[\"start\"],\n",
    "                    \"end\": segment[\"end\"],\n",
    "                    \"text\": result.text\n",
    "                })\n",
    "                \n",
    "                print(f\"Transcribed segment {i+1}/{len(segments)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error transcribing segment {i}: {e}\")\n",
    "    \n",
    "    # Format and display the transcript\n",
    "    print(\"\\nFull Transcript with Speaker Identification:\")\n",
    "    for segment in transcriptions:\n",
    "        print(f\"{segment['speaker']} ({segment['start']:.2f}s - {segment['end']:.2f}s): {segment['text']}\")\n",
    "    \n",
    "    # Optionally save the transcript to a file\n",
    "    import json\n",
    "    with open(\"transcript_with_speakers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(transcriptions, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Create a readable text version\n",
    "    with open(\"transcript_with_speakers.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for segment in transcriptions:\n",
    "            f.write(f\"{segment['speaker']} ({segment['start']:.2f}s - {segment['end']:.2f}s): {segment['text']}\\n\")\n",
    "    \n",
    "    print(\"\\nTranscript saved to transcript_with_speakers.json and transcript_with_speakers.txt\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in diarization or transcription process: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up temporary files\n",
    "import shutil\n",
    "if os.path.exists(\"temp_segments\"):\n",
    "    shutil.rmtree(\"temp_segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting speaker diarization...\n",
      "Initializing pipeline with token: hf_...QaZUB\n",
      "Using device: cuda\n",
      "Running diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/pyannote/audio/models/blocks/pooling.py:104: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1831.)\n",
      "  std = sequences.std(dim=-1, correction=1)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (4064) too large for available bit count (3928)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2432) too large for available bit count (2384)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3904) too large for available bit count (3352)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3392) too large for available bit count (3352)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1632) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3776) too large for available bit count (3352)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1696) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2112) too large for available bit count (1912)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1632) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2560) too large for available bit count (2200)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1920) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1952) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1632) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3968) too large for available bit count (3928)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1920) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3136) too large for available bit count (2776)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2368) too large for available bit count (2200)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (2080) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1952) too large for available bit count (1912)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (4000) too large for available bit count (3904)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (3872) too large for available bit count (3352)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1664) too large for available bit count (1624)\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1792) too large for available bit count (1624)\n",
      "/home/dpeykov/envs/jupyter1/lib/python3.12/site-packages/torchaudio/_backend/soundfile_backend.py:71: UserWarning: The MPEG_LAYER_III subtype is unknown to TorchAudio. As a result, the bits_per_sample attribute will be set to 0. If you are seeing this warning, please report by opening an issue on github (after checking for existing/closed ones). You may otherwise ignore this warning.\n",
      "  warnings.warn(\n",
      "[src/libmpg123/layer3.c:INT123_do_layer3():1774] error: part2_3_length (1888) too large for available bit count (1624)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 43 speech segments with 3 speakers\n",
      "Diarization results saved to diarization_results.json\n",
      "\n",
      "Generating full transcription...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/audio/transcriptions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full transcript saved to full_transcript.txt\n",
      "\n",
      "Full Transcript with Speaker Identification:\n",
      "SPEAKER_SPEAKER_00 (3.22s - 5.13s): Ня спуснах сега, вие трябва да добрите ме\n",
      "SPEAKER_SPEAKER_00 (7.89s - 9.30s): М\n",
      "SPEAKER_SPEAKER_00 (13.87s - 15.00s): Да\n",
      "SPEAKER_SPEAKER_00 (16.25s - 16.97s): Не се мютнахте нещо\n",
      "SPEAKER_SPEAKER_02 (16.94s - 16.96s): То самата мютва\n",
      "SPEAKER_SPEAKER_00 (18.10s - 21.73s): А, да\n",
      "SPEAKER_SPEAKER_02 (18.58s - 19.03s): То те мютва, да, да\n",
      "SPEAKER_SPEAKER_02 (20.97s - 21.02s): Кирил, Тони, мютнави\n",
      "SPEAKER_SPEAKER_00 (22.85s - 24.09s): Мютнави\n",
      "SPEAKER_SPEAKER_00 (31.44s - 31.98s): Аз сега го спирам и ще видя дали мога да го изтегля\n",
      "\n",
      "Transcript saved to transcript_with_speakers.json and transcript_with_speakers.txt\n",
      "Temporary files cleaned up\n",
      "/home/dpeykov/repos/Legal_project/video/Scoring -20250219_153216-Meeting Recording.mp3\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from openai import OpenAI\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "# Define file paths - remove spaces from filenames\n",
    "mp4_file = '/home/dpeykov/repos/Legal_project/video/Scoring -20250219_153216-Meeting Recording.mp3'\n",
    "mp3_file = mp4_file.replace('.mp4', '.mp3')\n",
    "\n",
    "# # First, let's fix the file permissions and rename files to remove spaces\n",
    "# try:\n",
    "#     # Check if the original file with spaces exists\n",
    "#     original_mp4 = '../video/Scoring -20250219_153216-Meeting Recording.mp4'\n",
    "#     if os.path.exists(original_mp4):\n",
    "#         # Create directory if it doesn't exist\n",
    "#         os.makedirs(os.path.dirname(mp4_file), exist_ok=True)\n",
    "        \n",
    "#         # Copy and rename the file (removing spaces)\n",
    "#         subprocess.run(['cp', original_mp4, mp4_file], check=True)\n",
    "        \n",
    "#         # Set proper permissions\n",
    "#         subprocess.run(['chmod', '755', mp4_file], check=True)\n",
    "#         print(f\"File copied and renamed to {mp4_file} with proper permissions\")\n",
    "#     else:\n",
    "#         print(f\"Original file {original_mp4} not found\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error setting up files: {e}\")\n",
    "\n",
    "# # Convert mp4 to mp3 using ffmpeg directly\n",
    "# try:\n",
    "#     # Use ffmpeg to extract audio from video\n",
    "#     subprocess.run(['ffmpeg', '-i', mp4_file, '-q:a', '0', '-map', 'a', mp3_file, '-y'], check=True)\n",
    "#     # Set proper permissions for the mp3 file\n",
    "#     subprocess.run(['chmod', '755', mp3_file], check=True)\n",
    "#     print(f\"Successfully converted {mp4_file} to {mp3_file}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error converting file: {e}\")\n",
    "\n",
    "# # Set up OpenAI client\n",
    "# api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# if not api_key:\n",
    "#     api_key = input(\"Enter your OpenAI API key: \")\n",
    "# client = OpenAI(api_key=api_key)\n",
    "\n",
    "# # Transcribe the full audio file first\n",
    "# print(\"Transcribing the full audio file...\")\n",
    "# try:\n",
    "#     with open(mp3_file, \"rb\") as audio_file:\n",
    "#         transcription = client.audio.transcriptions.create(\n",
    "#             model=\"whisper-1\", \n",
    "#             file=audio_file\n",
    "#         )\n",
    "#     print(\"Full transcription:\")\n",
    "#     print(transcription.text)\n",
    "    \n",
    "#     # Save the full transcription\n",
    "#     with open(\"full_transcription.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(transcription.text)\n",
    "    \n",
    "#     print(\"\\nFull transcription saved to full_transcription.txt\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during transcription: {e}\")\n",
    "\n",
    "# Attempt speaker diarization\n",
    "print(\"Attempting speaker diarization...\")\n",
    "try:\n",
    "    # Create temp directory for audio segments\n",
    "    os.makedirs(\"temp_segments\", exist_ok=True)\n",
    "    \n",
    "    # Get HF token\n",
    "    hf_token = \"hf_sIQaZUB\"  # Using the token from your logs\n",
    "    \n",
    "    print(f\"Initializing pipeline with token: {hf_token[:3]}...{hf_token[-5:]}\")\n",
    "    \n",
    "    # Initialize the pipeline with the token\n",
    "    pipeline = Pipeline.from_pretrained(\n",
    "        \"pyannote/speaker-diarization-3.1\",\n",
    "        use_auth_token=hf_token\n",
    "    )\n",
    "    \n",
    "    # Convert string to torch.device object\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Move the pipeline to the specified device\n",
    "    pipeline = pipeline.to(device)\n",
    "    \n",
    "    print(\"Running diarization...\")\n",
    "    # Run the pipeline on the audio file\n",
    "    diarization = pipeline(mp3_file)\n",
    "    \n",
    "    # Extract segments with speaker information\n",
    "    segments = []\n",
    "    for turn, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        segments.append({\n",
    "            \"speaker\": f\"SPEAKER_{speaker}\",\n",
    "            \"start\": turn.start,\n",
    "            \"end\": turn.end\n",
    "        })\n",
    "    \n",
    "    print(f\"Found {len(segments)} speech segments with {len(set(s['speaker'] for s in segments))} speakers\")\n",
    "    \n",
    "    # Sort segments by start time\n",
    "    segments.sort(key=lambda x: x[\"start\"])\n",
    "    \n",
    "    # Save the diarization results\n",
    "    with open(\"diarization_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(segments, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"Diarization results saved to diarization_results.json\")\n",
    "    \n",
    "    # Now we'll use the full transcription and align it with the diarization\n",
    "    print(\"\\nGenerating full transcription...\")\n",
    "    with open(mp3_file, \"rb\") as audio_file:\n",
    "        result = client.audio.transcriptions.create(\n",
    "            model=\"whisper-1\",\n",
    "            file=audio_file,\n",
    "            language=\"bg\"\n",
    "        )\n",
    "    \n",
    "    full_transcript = result.text\n",
    "    \n",
    "    # Save the full transcript\n",
    "    with open(\"full_transcript.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(full_transcript)\n",
    "    \n",
    "    print(\"Full transcript saved to full_transcript.txt\")\n",
    "    \n",
    "    # Create a simple alignment of the transcript with the speakers\n",
    "    # This is a simplified approach - for better results, you'd need more sophisticated alignment\n",
    "    \n",
    "    # Split the transcript into sentences\n",
    "    import re\n",
    "    sentences = re.split(r'[.!?]+', full_transcript)\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    # Distribute sentences among speakers based on timing\n",
    "    transcript_with_speakers = []\n",
    "    \n",
    "    # If we have more segments than sentences, we'll merge some segments\n",
    "    if len(segments) > len(sentences):\n",
    "        # Group segments by speaker\n",
    "        speaker_segments = {}\n",
    "        for segment in segments:\n",
    "            speaker = segment[\"speaker\"]\n",
    "            if speaker not in speaker_segments:\n",
    "                speaker_segments[speaker] = []\n",
    "            speaker_segments[speaker].append(segment)\n",
    "        \n",
    "        # Create a new list of merged segments\n",
    "        merged_segments = []\n",
    "        for speaker, segs in speaker_segments.items():\n",
    "            # Sort by start time\n",
    "            segs.sort(key=lambda x: x[\"start\"])\n",
    "            \n",
    "            # Merge consecutive segments\n",
    "            current_segment = None\n",
    "            for segment in segs:\n",
    "                if current_segment is None:\n",
    "                    current_segment = segment.copy()\n",
    "                elif segment[\"start\"] - current_segment[\"end\"] < 1.0:  # If less than 1 second gap\n",
    "                    current_segment[\"end\"] = segment[\"end\"]\n",
    "                else:\n",
    "                    merged_segments.append(current_segment)\n",
    "                    current_segment = segment.copy()\n",
    "            \n",
    "            if current_segment:\n",
    "                merged_segments.append(current_segment)\n",
    "        \n",
    "        # Sort merged segments by start time\n",
    "        merged_segments.sort(key=lambda x: x[\"start\"])\n",
    "        segments = merged_segments\n",
    "    \n",
    "    # Now distribute sentences among segments\n",
    "    sentence_idx = 0\n",
    "    for segment in segments:\n",
    "        if sentence_idx < len(sentences):\n",
    "            transcript_with_speakers.append({\n",
    "                \"speaker\": segment[\"speaker\"],\n",
    "                \"start\": segment[\"start\"],\n",
    "                \"end\": segment[\"end\"],\n",
    "                \"text\": sentences[sentence_idx]\n",
    "            })\n",
    "            sentence_idx += 1\n",
    "    \n",
    "    # Add any remaining sentences to the last speaker\n",
    "    while sentence_idx < len(sentences):\n",
    "        last_segment = segments[-1]\n",
    "        transcript_with_speakers.append({\n",
    "            \"speaker\": last_segment[\"speaker\"],\n",
    "            \"start\": last_segment[\"end\"],\n",
    "            \"end\": last_segment[\"end\"] + 2.0,  # Add 2 seconds\n",
    "            \"text\": sentences[sentence_idx]\n",
    "        })\n",
    "        sentence_idx += 1\n",
    "    \n",
    "    # Save the transcript with speakers\n",
    "    with open(\"transcript_with_speakers.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(transcript_with_speakers, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    with open(\"transcript_with_speakers.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for segment in transcript_with_speakers:\n",
    "            f.write(f\"{segment['speaker']} ({segment['start']:.2f}s - {segment['end']:.2f}s): {segment['text']}\\n\")\n",
    "    \n",
    "    print(\"\\nFull Transcript with Speaker Identification:\")\n",
    "    for segment in transcript_with_speakers:\n",
    "        print(f\"{segment['speaker']} ({segment['start']:.2f}s - {segment['end']:.2f}s): {segment['text']}\")\n",
    "    \n",
    "    print(\"\\nTranscript saved to transcript_with_speakers.json and transcript_with_speakers.txt\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in diarization or transcription process: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Clean up temporary files\n",
    "try:\n",
    "    if os.path.exists(\"temp_segments\"):\n",
    "        shutil.rmtree(\"temp_segments\")\n",
    "    print(\"Temporary files cleaned up\")\n",
    "except Exception as e:\n",
    "    print(f\"Error cleaning up temporary files: {e}\")\n",
    "\n",
    "print(mp3_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../video/Scoring-20250219_153216-Meeting_Recording.mp3'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp3_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
